---
title: "A Multivariate Approach to Modelling Lifestyle Risk Factors of Children Myopia in the US"
author: "Stephen Su, STATS 767"
output:
  pdf_document:
    fig_caption: yes
    number_sections: yes
  css: style.css
bibliography: refs.bib
csl: apa.csl
urlcolor: blue
---

<style type="text/css">

body {
  font-family: serif;
  text-align: justify;
}

</style>

```{r setup, include = FALSE}
library(knitr)
opts_chunk$set(
  message = FALSE, warning = FALSE, echo = FALSE, dpi = 110,
  fig.height = 3.5, fig.align = "center", comment = "#>",
  dev.args = list(png = list(type = "cairo"))
)
```
```{r lib}
library(tidyverse)
library(janitor)
library(GGally)
library(colorspace)
library(MASS)
library(DescTools)
library(MuMIn)
library(pROC)
```


# Introduction

## Background

The association between lifestyle factors and the development and subsequent progression of myopia among children has been long discussed and researched within the academic area. Among them, the Orinda Longitudinal Study of Myopia conducted research on children myopia spanning over 10 years. The research produced data that are both useful in exploring the lifestyle risk factors of myopia and a valuable case study for building and testing multivariate data models and analysis. This paper attempts to analyse the OLSM myopia dataset and produce a multivariate model.

## The Data

The dataset is from [ggeop/Myopia-Study](https://github.com/ggeop/Myopia-Study) [@ggeop], which is a subset from the original data collected in 1989-1990 and 2000-2001. The dataset consists of 618 observations and 17 variables. The main focus of the paper is around numeric, lifestyle-related (non-definitional) variables and the logical variable indicating the prevalence of myopia, which are represented by the following variables:

```{r data}
myopia <- readr::read_csv(
  "../data/myopia.csv",
  col_types = "__ld______ddddd___"
) %>%
  janitor::clean_names() %>%
  dplyr::mutate(across(
    where(is.logical),
    function(x) case_when(x ~ "Yes", !x ~ "No")
  ))

knitr::kable(
  data.frame(
    Variable_Name = names(myopia),
    Unit = c("boolean", "years", rep("hours per week", 5)),
    Description = c(
      "Myopia within the first five years of follow up",
      "Age at first visit",
      "Time spent engaging in sports/outdoor activities",
      "Time spent reading for pleasure",
      "Time spent playing video/computer games or working on the computer",
      "Time spent reading or studying for school assignments",
      "Time spent watching television"
    )
  )
)
```

Note: "Non-definitional" means the variables do not serve as an optometrical reference to myopia.

## Outputs and Deliverables

This paper and the project presentation only includes selective outputs serving as the final deliverable, including a **GGally** plot [@ggally] and model outputs from **base R** [@baseR] and the R **MASS** [@MASS] package. The detailed model building steps, intermediary models, and materials, such as the R source code, required to reproduce this paper can be found at the Github repository [szmsu2011/stats767proj](https://github.com/szmsu2011/stats767proj).

# Exploratory Analysis

A preliminary visualisation of the selected data suggests heavy right-skewness except for the numeric variable `age`. As a convention, a log-transformation to all numeric variables except for `age` attempts to mitigate the skewness, yet the minima of the variables are zero. Instead, the `log1p` transformation is applied across the variables. Nevertheless, a subsequent plot of the transformed data indicates the `log1p` transformation seems to over-correct the skewness of variables `sporthr` and `tvhr`. Therefore, a final decision is made to transform variables `readhr`, `comphr` and `studyhr` by $x \rightarrow log(1 + x)$ and `sporthr` and `tvhr` by $x \rightarrow \sqrt{x}$.

```{r explore, fig.cap = "The transformed data is a considerable improvement from the original, notwithstanding a substantial departure from normality. All subsequent discussions are based on the transformed data."}
(myopia_2 <- myopia %>%
  dplyr::mutate(
    across(4:6, log1p, .names = "log1p_{col}"),
    across(c(3, 7), sqrt, .names = "sqrt_{col}")
  ) %>%
  dplyr::select(-(3:7))) %>%
  GGally::ggpairs(
    ggplot2::aes(col = myopic),
    columns = 2:7,
    upper = list(continuous = wrap("cor", size = 2))
  ) +
  ggplot2::theme(strip.text.y = element_text(size = 5))
```

```{r effect, fig.cap = "The bivariate correlations with myopia of the numeric variables are relatively small."}
quantile_f <- function(p) {
  purrr::map(
    p,
    function(p) {
      eval(parse(
        text = paste0("function(x) quantile(x, probs = ", p, ", na.rm = TRUE)")
      ))
    }
  ) %>% purrr::set_names(p)
}

myopia_2 %>%
  tidyr::pivot_longer(-1, "var") %>%
  dplyr::group_by(myopic, var) %>%
  dplyr::summarise(across(value, quantile_f(seq(0, 1, .001)))) %>%
  dplyr::ungroup() %>%
  tidyr::pivot_longer(
    dplyr::starts_with(paste0("value", "_")),
    names_to = ".quantile",
    values_to = ".value"
  ) %>%
  dplyr::mutate(.quantile = 100 * as.numeric(
    gsub(paste0("value", "_"), "", .quantile)
  )) %>%
  ggplot(aes(myopic, .value, col = .quantile, group = .quantile)) +
  geom_point() +
  geom_line() +
  ggplot2::facet_wrap(~var, scales = "free_y") +
  ggplot2::labs(
    title = "Bivariate Association with Myopia",
    col = "Quantile", x = "Is Myopic", y = ""
  ) +
  colorspace::scale_colour_continuous_diverging(
    mid = 50, c1 = 120
  )
```

A quantile plot, which is an extension to a boxplot, indicates a weak positive bivariate correlation with `log1p_readhr` and `log1p_studyhr` to myopia as well as a negative correlation with `sqrt_sporthr`. The factors affecting myopia development include complicated aspects, including lifestyle, gene, other conditions and their complications. Therefore, without surprise, bivariate correlations of each variable are small, which is typical in medical statistics. A pairs plot shows only a moderate correlation between `log1p_studyhr` and `age` ($\rho \approx 0.412$); that is, the expected study hours of typical children increase with age. In conjunction with all the variance inflation factors being below 1.5, it is reasonable to dismiss the concern of multicollinearity. The within-group variances are similar, even with the `age` coincidentally, except for the two square-rooted variables. The equality of covariance is not satisfied.

# Methodologies and Model Diagnostics

## Candidate Models

Upon approaching multivariate data, intuitively, Principal Component Analysis, a dimension reduction technique, is considered first. Principal Component Analysis (abbr. PCA) produces a linear space with a dimension equalling to the rank of the design matrix $\mathbf{X}$, by consecutively maximising the variance of each principal component under the constraint that the sum of squares of coefficients equal one and each principal component is orthogonal to all previous ones. As the most fundamental multivariate statistical model, PCA does not preserve the relationship of the variables and thus ignores the features other than the maximal variance of the principal components upon dimension reduction. As a result, PCA might not separate the myopic groups well enough in achieving the main objectives.

As the objective is in an attempt to model and classify a single categorical variable with a set of numeric variables, The Linear Discriminant Analysis (abbr. LDA) is an alternative to PCA. Similar to PCA, LDA is a dimension reduction technique producing linear discriminant functions - linear combinations of the original variables. Instead of maximising the variability of each dimension, LDA attempts to maximise the *ANOVA F-statistic*, which represents the ratio of the between-group variance against the within-group variance. The maximum dimension of linear discriminant functions is one less of the number of levels. The implication for the myopic category is a single-dimension linear discriminant function. Also, LDA is particularly suitable for data with numeric variables on the same *natural* scale (i.e., the units of variables), which is not the case due to different transformations. The classification of categories is based on the Bayes Discriminant Rule, which compares the posterior probability, satisfying

\begin{equation} f(\theta_i | \mathbf{x}) \propto f(\theta_i)f(\mathbf{x} | \theta_i) = \pi_if(\mathbf{x}) \end{equation}

$\pi_i$ represents the prior probability of developing myopia during childhood, $f(\mathbf{x})$ is the likelihood (conditional probabilities given the myopia status) from the sample. Without specifying the prior probability, the `lda()` function assumes a prior equalling sample proportion [@MASS], which is probably not a prior potentially giving the most accurate predictions. On the other hand, a US research on children myopia suggests a prevalence of 24% [@preval18]. As such, the first LDA fit takes the prior (0.76, 0.24). Nevertheless, it is precarious to assume this is the correct prior, and adjustments may be needed upon evaluating the predictive power of the model, including techniques like cross-validation.

Other alternative multivariate models to PCA and LDA are Quadratic Discriminant Analysis (abbr. QDA) and PLS-Discriminant Analysis (abbr. PLS-DA). QDA relaxes the assumption of equality of within-group covariance required for LDA by separating the groups with a quadratic surface. Under the expectation that the equality of covariance is violated, QDA may slightly out-perform and is potentially a better fit. Nonetheless, as QDA models the whole covariance matrix without dimension reduction as well as the use of quadratic combinations, the selective interpretation of loadings for PCA and LDA is not possible. On the contrary, PLS-DA is a comprise of LDA towards PCA in rank deficiency and is probably unneeded.

It is tempting to compare the predictive power of the final model (from STATS 767) to other commonly used statistical models. As the question of interest is to model a Bernoulli (Yes/No) categorical variable with a set of numeric variables, the classical frequentist approach is multiple logistic regression. Hence, the performance of the final multivariate model (LDA) is compared against multiple logistic regression.

## Model Fitting and Evaluation

This section compares the performance of LDA and QDA using leave-1-out cross-validation. A prior of (0.76, 0.24) is used for the preliminary models for evaluation and subsequent adjustments.

The results of leave-1-out cross-validation show a high specificity yet extremely low sensitivity for both LDA and QDA, notwithstanding a high overall correct classification rate, at (0.99, 0, 0.86) for LDA and (0.97, 0.02, 0.84) for QDA. The ridiculous results are partially due to the low sample likelihood of being myopic. Besides, the setting of the prior is also a major contributing factor, as $sensitivity \rightarrow 1$ as $\pi_1 \rightarrow 1$ yet $specificity \rightarrow 1$ as $\pi_0 \rightarrow 1$. On the other hand, $\mathbb{P}(Type\,I) \rightarrow 1$ as $\pi_1 \rightarrow 1$ and $\mathbb{P}(Type\,II) \rightarrow 1$ as $\pi_0 \rightarrow 1$. Therefore, instead of sole sensitivity or specificity, the goal is to search for a prior, which minimises the *balanced error rate*. Achieving such is equivalent to maximising the value of $sensitivity + specificity$, also known as the *sensitivity-specificity trade-off*. The area under curve of the *receiver operating characteristic* (ROC) curve computed using **DescTools** [@Desc] of LDA and QDA are 0.628 and 0.672, respectively, which are relatively similar. Given that both models possess similar predictive power, yet LDA allows selective interpretation of loadings which QDA does not, the decision is to select LDA, under a prior such that the *BER* is minimised, as the final model.

```{r eval, fig.cap = "The receiver operating characteristic (ROC) curve of LDA"}
roc <- function(data, y, .f, p) {
  y <- substitute(y)

  tn_and_tp <- purrr::map(
    p,
    function(x) {
      fit <- .f(
        formula(paste(deparse(y), "~ .")),
        data = data,
        prior = c(1 - x, x)
      )
      cm <- table(
        data[[deparse(y)]],
        predict(fit)[["class"]]
      )
      diag(cm) / rowSums(cm)
    }
  ) %>%
    purrr::set_names(seq_len(length(p))) %>%
    dplyr::bind_rows()

  i_max <- which.max(rowSums(tn_and_tp))

  print(tn_and_tp %>%
    ggplot(aes(x = No, y = Yes)) +
    geom_line(col = "steelblue", lwd = 1) +
    ggplot2::scale_x_reverse() +
    geom_abline(slope = 1, intercept = 1) +
    geom_point(
      data = tn_and_tp[i_max, ],
      size = 2,
      col = "red"
    ) +
    geom_text(
      data = tn_and_tp[i_max, ],
      label = paste("p =", p[i_max]),
      position = position_nudge(x = .02, y = -.02),
      col = "red",
      hjust = 0
    ) +
    ggplot2::labs(
      x = "Specificity",
      y = "Sensitivity"
    ) +
    ggplot2::theme_bw())

  list(
    sensitivity = as.numeric(tn_and_tp[i_max, 2]),
    specificity = as.numeric(tn_and_tp[i_max, 1]),
    auc = with(tn_and_tp, DescTools::AUC(No, Yes)),
    p_optim = p[i_max]
  )
}

roc(myopia_2, myopic, MASS::lda, p = seq(.001, .999, .001))
```

The optimal prior $(\pi_0, \pi_1)$ minimising *BER* is (0.508, 0.492), which is close to a uniform prior, giving a sensitivity of 0.593 and specificity of 0.618. The area under curve of the *ROC* curve is 0.628, indicating a weak-moderate predictive power ($AUC = 0.5$ is a random predictor). Again, this is typical and unsurprising in medical statistics. A weak relationship is claimable between lifestyle and myopia.

## Model Output and Interpretation

```{r out, fig.cap = "The plot of the linear discriminant scores"}
myopia_lda <- MASS::lda(myopic ~ ., data = myopia_2, prior = c(.508, .492))

dplyr::bind_cols(
  predict(myopia_lda)[["x"]],
  myopic = myopia_2[["myopic"]]
) %>%
  ggplot(aes(x = LD1, y = myopic)) +
  geom_boxplot(outlier.alpha = 0) +
  geom_jitter(shape = 1, height = .02) +
  ggplot2::labs(y = "Is Myopic")

cor(dplyr::bind_cols(
  as_tibble(predict(myopia_lda)[["x"]]),
  myopia_2[, 2:7]
))[-1, 1, drop = FALSE] %>%
  structure(class = "loadings")
```

As the categorical variable is Bernoulli, there exists up to and only one dimension of linear discriminant function. The scatter-box plot of the one-dimensional linear discriminant scores indicates that `LD1` is higher in myopic children than not. The loadings of linear discriminant function show a weak positive correlation with `age`, moderate positive correlation with `log1p_readr` and high negative correlation with `sqrt_sporthr`. The implication is that the risk of myopia increases mildly with age, is moderately associated with a longer focused reading time, and strongly (negatively) associated with time spent in engaging with sports or outdoor activities. Although such findings cannot establish causation, they are on par with our typical belief about myopia. Surprisingly, the relationship of the time spent on studying, computer and television to myopia is unclear, given the linear discriminant scores and loadings.

# Comparison with Classical Frequentist Approach

## The Multiple Logistic Regression

Upon approaching a Bernoulli random variable of interest given a set of numeric explanatory variables, one would immediately think of a multiple logistic regression, whose model expression is given by

\begin{equation} \mathbf{Y}\overset{iid}{\sim}Bernoulli(\pmb{\theta})\;|\;logit(\pmb{\theta})=\mathbf{X}\pmb{\beta};\;\pmb{\theta}\in\mathbb{R}^n,\;\pmb{\beta}\in\mathbb{R}^p,\;\mathbf{X}\in\mathbb{R}^n\times\mathbb{R}^p \end{equation}

A *full logistic regression model* `Y ~ .` is fitted, and the information-theoretic is adopted by searching every sub-model of the full model exhaustively and select the best model ranked by *AICc* [@MuMIn]. As of the LDA findings of linear subspace, quadratic transformation is unnecessary. The best model is

```{r logis}
myopia_logis_ini <- glm(factor(myopic) ~ ., data = myopia_2, family = "binomial")

oo <- options(na.action = "na.fail")
logis_all <- MuMIn::dredge(myopia_logis_ini, rank = "AICc")
options(oo)

myopia_logis <- glm(
  MuMIn::get.models(logis_all, 1)[[1]][["formula"]],
  data = myopia_2,
  family = "binomial"
)

summary(myopia_logis)
```

The *residual deviance* ($\mathcal{D} \approx 466.43$) is less than the model’s degrees of freedom ($\nu = 615$), which dismisses the concern of an inadequate fit (overdispersion). The model provides moderate evidence for a positive association between time spent in focused reading and myopia and very strong evidence for a negative association between time spent in engaging with sports or outdoor activities and myopia. The findings from the logistic regression partially agree with LDA.

```{r logisroc, fig.cap = "The receiver operating characteristic (ROC) curve of the multiple logistic regression"}
roc_logis <- pROC::roc(
  response = myopia_2[["myopic"]],
  predictor = fitted(myopia_logis)
)

tn_and_tp <- with(roc_logis, tibble(
  Specificity = specificities,
  Sensitivity = sensitivities
))

i_max <- which.max(rowSums(tn_and_tp))

tn_and_tp %>%
  ggplot(aes(
    x = sort(Specificity, decreasing = TRUE),
    y = sort(Sensitivity)
  )) +
  geom_line(col = "steelblue", lwd = 1) +
  ggplot2::scale_x_reverse() +
  geom_abline(slope = 1, intercept = 1) +
  geom_point(
    data = tn_and_tp[i_max, ],
    size = 2,
    col = "red"
  ) +
  geom_text(
    data = tn_and_tp[i_max, ],
    label = paste("c =", round(
      roc_logis[["thresholds"]][i_max], 3
    )),
    position = position_nudge(x = .02, y = -.02),
    col = "red",
    hjust = 0
  ) +
  ggplot2::labs(
    x = "Specificity",
    y = "Sensitivity"
  ) +
  ggplot2::theme_bw()

list(
  sensitivity = as.numeric(tn_and_tp[i_max, 2]),
  specificity = as.numeric(tn_and_tp[i_max, 1]),
  auc = as.numeric(roc_logis[["auc"]]),
  c_optim = roc_logis[["thresholds"]][i_max]
)
```

Using **pROC** package [@pR], the multiple logistic regression, at the optimal threshold for $\theta$: $c_\theta \approx 0.156$, gives a sensitivity of 0.469 and specificity of 0.726, with an area under curve of *ROC* curve of 0.618, which is similar but slightly less than LDA and QDA. The classical frequentist method possesses similar yet ever so slightly weaker predictive power than the two multivariate ones.

## Linear Discriminant Analysis cf. Multiple Logistic Regression

To avoid over-fitting, the information-theoretic approach to *GLMs* penalises additional explanatory variables to preserve degrees of freedom, which reflects upon *AICc* (similar to backward elimination of nonsignificant covariates). The multiple logistic regression, the *Binomial* family of *GLM*, adopts an include-or-exclude approach to covariates, completely dropping “unuseful” explanatory variables.

On the contrary, linear discriminant functions are linear combinations of all the variables, with weights such that the *ANOVA F-statistic* is maximised under certain constraints. Such an approach takes into account the information from all (useful and useless) variables while assigning weights (LD coefficients) to them, which avoids the dropping of less significant variables yet with useful information completely.

Both LDA and the multiple logistic regression agree upon the significance of reading and outdoor time on children myopia, and the exception is age. While the multiple logistic regression drops all the other covariates, the linear discriminant loadings and coefficients of the numeric variables are non-zero. As the paper repeatedly emphasises, confounding effects of variables in medical statistics are extremely complicated, such that every single variable is potentially useful (or useless). Provided that the dataset has a considerably limited number of variables, dropping any variable should be considered cautiously.

# Conclusion

The risk of developing myopia for children increases with age and time spent in focused reading, while the time spent on sports or outdoor activities is a negative risk factor. Such findings appear to coincide with our typical belief in children myopia. However, the relationship between computer and television use time and myopia is unclear. The *minimal-BER* LDA model possesses a weak-moderate predictive power with a sensitivity of 0.593 and a specificity of 0.618, as well as an area under curve of the *ROC* curve of 0.628. The dataset is better served for building an explanatory model than a predictive model.

# Further Research

Recalling the linear discriminant loadings from the LDA model, the linear discriminant function, `LD1`, *appears* to be a measure of an “unhealthy lifestyle index”, such that it appears to be correlated with perceivably unhealthy behaviours. Although such a claim is arguable, such as if studying or reading for too long is unhealthy remains highly questionable, as well as the need for additional lifestyle-reflecting explanatory variables, a hypothesis can be drawn from the preliminary findings for future research:

\begin{equation} An\;overall\;unhealthy\;lifestyle\;is\;associated\;with\;the\;risk\;of\;developing\;myopia\;during\;childhood. \end{equation}

In addition, Dr Beatrix Jones kindly advised the plausibility of modelling the relationship between the set of lifestyle-related variables and the prescription variables, such as Spherical-Equivalent Refraction (SPHEQ). Nonetheless, the majority of the sample has a spherical lens of below 0.75 (in absolute value), classified as non-myopic, as well as the maximum of SPHEQ is below 2, most of the variability of SPHEQ is meaningless for modelling myopia. On the contrary, LDA’s factorisation and categorisation of SPHEQ remove the meaningless variation. Therefore, Canonical Correlation (abbr. CCorA) and PLS models are unsuitable for the data. Instead, they should be applied to studies for the *progression* of myopia; in the context that the data is sampled from myopic individuals of different degrees (e.g., low and high).

# Reproducing The Paper

The paper can be reproduced with **rmarkdown** [@rmd] from [report.Rmd]( https://github.com/szmsu2011/stats767proj/blob/main/report/report.Rmd). Please [install the required packages]( https://github.com/szmsu2011/stats767proj/blob/main/README.md) and their dependencies prior to knitting the document. Initial exploratory analyses, model building and diagnostics can be found in [szmsu2011/stats767proj/prelim]( https://github.com/szmsu2011/stats767proj/tree/main/prelim).

\newpage

# Bibliography
