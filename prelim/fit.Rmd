---
title: "STATS 767 Project: Model Fitting"
author: "Stephen Su, ID 253222842"
output:
  html_document:
    code_folding: hide
    df_print: paged
    fig_caption: yes
    number_sections: yes
  css: style.css
urlcolor: blue
---

<style type="text/css">

body {
  font-family: serif;
  text-align: justify;
}

</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, dev.args = list(type = "cairo-png"))
```
```{r warning=FALSE,message=FALSE}
library(tidyverse)
library(janitor)
library(MASS)
library(DescTools)
library(mixOmics)
library(MuMIn)
library(pROC)

myopia <- readr::read_csv(
  "../data/myopia.csv",
  col_types = "__ld______ddddd___"
) %>%
  janitor::clean_names() %>%
  dplyr::mutate(across(
    where(is.logical),
    function(x) case_when(x ~ "Yes", !x ~ "No")
  ))
myopia_2 <- myopia %>%
  dplyr::mutate(
    across(4:6, log1p, .names = "log1p_{col}"),
    across(c(3, 7), sqrt, .names = "sqrt_{col}")
  ) %>%
  dplyr::select(-(3:7))
```

# Linear/Quadratic Discriminant Analysis
```{r}
myopia_lda <- MASS::lda(myopic ~ ., data = myopia_2, prior = c(.76, .24))
myopia_qda <- MASS::qda(myopic ~ ., data = myopia_2, prior = c(.76, .24))
```

## LDA Model Visualisation
```{r fig.align='center'}
dplyr::bind_cols(
  predict(myopia_lda)[["x"]],
  myopic = myopia_2[["myopic"]]
) %>%
  ggplot(aes(x = LD1, y = myopic)) +
  geom_boxplot(outlier.shape = 1) +
  ggplot2::labs(title = "LD Score Plot")
```

## LDA Loadings
```{r}
cor(dplyr::bind_cols(
  as_tibble(predict(myopia_lda)[["x"]]),
  myopia_2[, 2:7]
))[-1, 1, drop = FALSE] %>%
  structure(class = "loadings")
```

## Model Prediction and Evaluation

### Leave-1-Out Cross-Validation
```{r}
get_cm <- function(data, y, .f, prior) {
  y <- substitute(y)

  table(
    actual = data[[deparse(y)]],
    predicted = purrr::map(
      seq_len(nrow(data)),
      function(i) {
        fit <- .f(
          formula(paste(deparse(y), "~ .")),
          data = data[-i, ],
          prior = prior
        )
        as.character(predict(fit, data[i, ])[["class"]])
      }
    ) %>%
      purrr::set_names(seq_len(nrow(data))) %>%
      dplyr::bind_rows() %>%
      t()
  )
}

cat("MASS::lda")
get_cm(myopia_2, myopic, MASS::lda, prior = c(.76, .24))
cat("MASS::qda")
get_cm(myopia_2, myopic, MASS::qda, prior = c(.76, .24))
```
The models have extremely high specificity but low sensitivity, could be the priors.

### The Sensitivity/Specificity Trade-Off
```{r fig.align='center'}
roc <- function(data, y, .f, p) {
  y <- substitute(y)

  tn_and_tp <- purrr::map(
    p,
    function(x) {
      fit <- .f(
        formula(paste(deparse(y), "~ .")),
        data = data,
        prior = c(1 - x, x)
      )
      cm <- table(
        data[[deparse(y)]],
        predict(fit)[["class"]]
      )
      diag(cm) / rowSums(cm)
    }
  ) %>%
    purrr::set_names(seq_len(length(p))) %>%
    dplyr::bind_rows()

  i_max <- which.max(rowSums(tn_and_tp))

  print(tn_and_tp %>%
    ggplot(aes(x = No, y = Yes)) +
    geom_line(col = "steelblue", lwd = 1) +
    ggplot2::scale_x_reverse() +
    geom_abline(slope = 1, intercept = 1) +
    geom_point(
      data = tn_and_tp[i_max, ],
      size = 2,
      col = "red"
    ) +
    geom_text(
      data = tn_and_tp[i_max, ],
      label = paste("p =", p[i_max]),
      position = position_nudge(x = .02, y = -.02),
      col = "red",
      hjust = 0
    ) +
    ggplot2::labs(
      title = paste(
        "ROC Curve of",
        deparse(substitute(.f))
      ),
      x = "Specificity",
      y = "Sensitivity"
    ) +
    ggplot2::theme_bw())

  list(
    sensitivity = as.numeric(tn_and_tp[i_max, 2]),
    specificity = as.numeric(tn_and_tp[i_max, 1]),
    auc = with(tn_and_tp, DescTools::AUC(No, Yes)),
    p_optim = p[i_max]
  )
}

roc(myopia_2, myopic, MASS::lda, p = seq(.001, .999, .001))
roc(myopia_2, myopic, MASS::qda, p = seq(.001, .999, .001))
```

# Principal Component Analysis
```{r fig.align='center'}
myopia_pc <- prcomp(myopia_2[-1], scale = TRUE)

tibble(
  PC_n = paste0("PC", 1:6),
  vari = myopia_pc[["sdev"]]^2
) %>%
  ggplot(aes(x = ordered(PC_n, unique(PC_n)), y = vari)) +
  geom_bar(stat = "identity") +
  geom_hline(yintercept = 1, col = "red") +
  ggplot2::labs(
    title = "Screeplot of Principal Components",
    x = "",
    y = expression(sigma^2)
  )
```
2 or 3 principal components would be sufficient as per the screeplot.
```{r}
cumsum(
  purrr::map_dbl(
    (s2 <- myopia_pc[["sdev"]]^2),
    function(x) x / sum(s2)
  )
)
```
2 or 3 principal components explain 46.96% or 63.84% of the total variability.
```{r fig.align='center'}
dplyr::bind_cols(
  myopia_pc[["x"]],
  myopic = myopia_2[["myopic"]]
) %>%
  ggplot(aes(
    x = PC1,
    y = PC2,
    col = myopic
  )) +
  geom_point() +
  ggplot2::stat_ellipse(
    level = .5,
    lwd = 1
  ) +
  ggplot2::labs(title = "PC Score Plot")
```
PCA is doing quite poorly in separating the group of myopic people from the normal.

# Partial Least Squares for Discriminant Analysis (PLS-DA)
```{r fig.align='center'}
myopia_plsda <- mixOmics::plsda(myopia_2[-1], factor(myopia_2[["myopic"]]), 6)

mixOmics::plotIndiv(
  myopia_plsda,
  comp = 1:2,
  group = myopia_2[["myopic"]],
  ind.names = FALSE,
  legend = TRUE,
  ellipse = TRUE,
  ellipse.level = .5
)

plot(mixOmics::perf(myopia_plsda, validation = "loo"))

mixOmics::get.confusion_matrix(
  myopia_2[["myopic"]],
  predicted = predict(
    myopia_plsda,
    myopia_2[-1],
    dist = "max.dist"
  )[["class"]][["max.dist"]][, 2]
)
```
Classification rate with max.dist is comparable with LDA, but cannot specify priors.

# Logistic Regression
```{r fig.align='center',message=FALSE}
myopia_logis_ini <- glm(factor(myopic) ~ ., data = myopia_2, family = "binomial")

oo <- options(na.action = "na.fail")
logis_all <- MuMIn::dredge(myopia_logis_ini, rank = "AICc")
options(oo)

(myopia_logis <- glm(
  MuMIn::get.models(logis_all, 1)[[1]][["formula"]],
  data = myopia_2,
  family = "binomial"
)) %>%
  summary()

roc_logis <- pROC::roc(
  response = myopia_2[["myopic"]],
  predictor = fitted(myopia_logis)
)

tn_and_tp <- with(roc_logis, tibble(
  Specificity = specificities,
  Sensitivity = sensitivities
))

i_max <- which.max(rowSums(tn_and_tp))

tn_and_tp %>%
  ggplot(aes(
    x = sort(Specificity, decreasing = TRUE),
    y = sort(Sensitivity)
  )) +
  geom_line(col = "steelblue", lwd = 1) +
  ggplot2::scale_x_reverse() +
  geom_abline(slope = 1, intercept = 1) +
  geom_point(
    data = tn_and_tp[i_max, ],
    size = 2,
    col = "red"
  ) +
  geom_text(
    data = tn_and_tp[i_max, ],
    label = paste("c =", round(
      roc_logis[["thresholds"]][i_max], 3
    )),
    position = position_nudge(x = .02, y = -.02),
    col = "red",
    hjust = 0
  ) +
  ggplot2::labs(
    title = "ROC Curve of Logistic Regression",
    x = "Specificity",
    y = "Sensitivity"
  ) +
  ggplot2::theme_bw()

list(
  sensitivity = as.numeric(tn_and_tp[i_max, 2]),
  specificity = as.numeric(tn_and_tp[i_max, 1]),
  auc = as.numeric(roc_logis[["auc"]]),
  c_optim = roc_logis[["thresholds"]][i_max]
)
```
